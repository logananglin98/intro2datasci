<?xml version="1.0" encoding="UTF-8"?>

<chapter xml:id="ch13-word-perfect" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Word Perfect</title>

  <section xml:id="word-clouds-and-r-packages">
    <title>Word Clouds and R Packages</title>

  <introduction>
              <image source="data-science-cloud.png"/>
    				<p><em>
					The picture at the start of this chapter is a so-called "<idx>word cloud</idx><term>word cloud</term>" that was generated by examining all of the words returned from a Twitter search of the term "data science." (This one was made using a web application at <url href="http://www.jasondavies.com">http://www.jasondavies.com</url>.) These colorful word clouds are fun to look at, but they also do contain some useful information. The geometric arrangement of words on the figure is partly random and partly designed and organized to please the eye. Same with the colors. The font size of each word, however, conveys some measure of its importance in the "<idx>corpus</idx><term>corpus</term>" of words that was presented to the word cloud graphics program. Corpus, from the Latin word meaning "body," is a word that text analysts use to refer to a body of text material, often consisting of one or more documents. When thinking about a corpus of textual data, a set of documents could really be anything: web pages, word processing documents on your computer, a set of Tweets, or government reports. In most cases, text analysts think of a collection of documents, each of which contains some natural language text, as a corpus if they plan to analyze all the documents together.
            </em></p>
  </introduction>

  


				<p>
					The word cloud on the previous page shows that "Data" and "Science" are certainly important terms that came from the search of Twitter, but there are dozens and dozens of less important, but perhaps equally interesting, words that the search results contained. We see words like algorithms, molecules, structures, and research, all of which could make sense in the context of data science. We also see other terms, like #christian, Facilitating, and Coordinator, that don’t seem to have the same obvious connection to our original search term "data science." This small example shows one of the fundamental challenges of natural language processing and the closely related area of search: ensuring that the analysis of text produces results that are relevant to the task that the user has in mind.
				</p>

				<p>
					In this chapter we will use some new R packages to extend our abilities to work with text and to build our own word cloud from data retrieved from Twitter. If you have not worked on the chapter "String Theory" that precedes this chapter, you should probably do so before continuing, as we build on the skills developed there.
				</p>
      </section>

      <section xml:id="ch13-data-prep">
    <title>Preparing Tweet Data for Analysis</title>
				<p>
					Depending upon where you left off after the previous chapter, you will need to retrieve and pre-process a set of tweets, using some of the code you already developed, as well as some new code. At the end of the previous chapter, we have provided sample code for the TweetFrame() function, that takes a search term and a maximum tweet limit and returns a time-sorted dataframe containing tweets. Although there are a number of comments in that code, there are really only three lines of functional code thanks to the power of the twitteR package to retrieve data from Twitter for us. For the activities below, we are still working with the dataframe that we retrieved in the previous chapter using this command:
				</p>

				<p>
					tweetDF &lt;-TweetFrame("#solar",100)
				</p>

				<p>
					This yields a dataframe, tweetDF, that contains 100 tweets with the hashtag #solar, presumably mostly about solar energy and related "green" topics. Before beginning our work with the two new R packages, we can improve the quality of our display by taking out a lot of the junk that won’t make sense to show in the word cloud. To accomplish this, we have authored another function that strips out extra spaces, gets rid of all URL strings, takes out the retweet header if one exists in the tweet, removes hashtags, and eliminates references to other people’s Twitter handles. For all of these transformations, we have used string replacement functions from the stringr package that was introduced in the previous chapter. As an example of one of these transformations, consider this command, which appears as the second to last line of the CleanTweet() function:
				</p>

				<p>
					tweets &lt;- str_replace_all(tweets,"@[a-z,A-Z]*","")
				</p>

				<p>
					You should feel pretty comfortable reading this line of code, but if not, here’s a little more practice. The left hand side is easy: we use the assignment arrow to assign the results of the right hand side expression to a data object called "tweets." Note that when this statement is used inside the function as shown at the end of the chapter, "tweets" is a temporary data object that is used just within CleanTweets() after which it disappears automatically.
				</p>

				<p>
					The right hand side of the expression uses the <term>str_replace_all()</term> function from the stringr package. We use the "all" function rather than str_replace() because we are expecting multiple matches within each individual tweet. There are three arguments to the str_replace_all() function. The first is the input, which is a vector of character strings (we are using the temporary data object "tweets" as the source of the text data as well as its destination), the second is the regular expression to match, and the third is the string to use to replace the matches, in this case the empty string as signified by two double quotes with nothing between them. The regular expression in this case is the at sign, "@", followed by zero or more upper and lowercase letters. The asterisk, "*", after the stuff in the square brackets is what indicates the zero or more. That regular expression will match any screenname referral that appears within a tweet.
				</p>

				<p>
					If you look at a few tweets you will find that people refer to each other quite frequently by their screennames within a tweet, so @SolarFred might occur from time to time within the text of a tweet.
				</p>

				<p>
					Here’s something you could investigate on your own: Can screennames contain digits as well as letters? If so, how would you have to change the regular expression in order to also match the digits zero through nine as part of the screen name? On a related note, why did we choose to strip these screen names out of our tweets? What would the word cloud look like if you left these screennames in the text data?
				</p>

				<p>
					Whether you typed in the function at the end of this chapter or you plan to enter each of the cleaning commands individually, let’s begin by obtaining a separate vector of texts that is outside the original dataframe:
				</p>

				<p>
					&gt; cleanText &lt;- tweetDF$text
				</p>

				<p>
					&gt; head(cleanText, 10)
				</p>

				<p>
					There’s no critical reason for doing this except that it will simplify the rest of the presentation. You could easily copy the tweetDF$text data into another column in the same dataframe if you wanted to. We’ll keep it separate for this exercise so that we don’t have to worry about messing around with the rest of the dataframe. The head() command above will give you a preview of what you are starting with. Now let’s run our custom cleaning function:
				</p>

				<p>
					&gt; cleanText&lt;-CleanTweets(cleanText)
				</p>

				<p>
					&gt; head(cleanText, 10)
				</p>

				<p>
					Note that we used our "cleanText" data object in the first command above as both the source and the destination. This is an old computer science trick for cutting down on the number of temporary variables that need to be used. In this case it will do exactly what we want, first evaluating the right hand side of the expression by running our CleanTweets() function with the cleanText object as input and then taking the result that is returned by CleanTweets() and assigning it back into cleanText, thus overwriting the data that was in there originally. Remember that we have license to do whatever we want to cleanText because it is a copy of our original data, and we have left the original data intact (i.e., the text column inside the tweetDF dataframe).
				</p>
</section>

  <section xml:id="ch13-text-mining-tm">
    <title>Text Mining with the tm Package</title>
				<p>
					The head() command should now show a short list of tweets with much of the extraneous junk filtered out. If you have followed these steps, cleanText is now a vector of character strings (in this example exactly 100 strings) ready for use in the rest of our work below. We will now use the "tm" package to process our texts. The "tm" in this case refers to "<idx>text mining</idx><term>text mining</term>," and is a popular choice among the many text analysis packages available in R. By the way, text mining refers to the practice of extracting useful analytic information from corpora of text (corpora is the plural of corpus). Although some people use text mining and natural language processing interchangeably, there are probably a couple subtle differences worth considering. First, the "mining" part of text mining refers to an area of practice that looks for unexpected patterns in large data sets, or what some people refer to as knowledge discovery in databases. In contrast, natural language processing reflects a more general interest in understanding how machines can be programmed (or learn on their own) how to digest and make sense of human language. In a similar vein, text mining often focuses on statistical approaches to analyzing text data, using strategies such as counting word frequencies in a corpus. In natural language processing, one is more likely to hear consideration given to linguistics, and therefore to the processes of breaking text into its component grammatical pieces such as nouns and verbs. In the case of the "tm" addon package for R, we are definitely in the statistical camp, where the main process is to break down a corpus into sequences of words and then to tally-up the different words and sequences we have found.
				</p>

				<p>
					To begin, make sure that the tm package is installed and "libraryed" in your copy of R and R-Studio. You can use the graphic interface in R-Studio for this purpose or the EnsurePackage() function that we wrote in a previous chapter. Once the tm package is ready to use, you should be able to run these commands:
				</p>

				<p>
					&gt; tweetCorpus&lt;-Corpus(VectorSource(cleanText))
				</p>

				<p>
					&gt; tweetCorpus
				</p>

				<p>
					A corpus with 100 text documents
				</p>

				<p>
					&gt; tweetCorpus&lt;-tm_map(tweetCorpus, tolower)
				</p>

				<p>
					&gt; tweetCorpus&lt;-tm_map(tweetCorpus, removePunctuation)
				</p>

				<p>
					&gt; tweetCorpus&lt;-tm_map(tweetCorpus,removeWords,+
				</p>

				<p>
					stopwords('english'))
				</p>

				<p>
					In the first step above , we "coerce" our cleanText vector into a custom "Class" provided by the tm package and called a "Corpus," storing the result in a new data object called "tweetCorpus." This is the first time we have directly encountered a "<idx>class</idx><term>Class</term>." The term "<idx>object oriented programming</idx><term>object oriented programming</term>." comes from an area of computer science called "object oriented programming." Although R is different in many ways from object-oriented languages such as Java or C++, it does contain many of the most fundamental features that define an object-oriented language. For our purposes here, there are just a few things to know about a class. First, a class is nothing more or less than a definition for the structure of a data object. Second, classes use basic data types, such as numbers, to build up more complex data structures. For example, if we made up a new "Dashboard" class, it could contain one number for "Miles Per Hour," another number for "RPM," and perhaps a third one indicating the remaining "Fuel Level." That brings up another point about Classes: users of R can build their own. In this case, the author of the tm package, Ingo Feinerer, created a new class, called Corpus, as the central data structure for text mining functions. (Feinerer is a computer science professor who works at the Vienna University of Technology in the Database and Artificial Intelligence Group.) Last, and most important for this discussion, a Class not only contains definitions about the structure of data, it also contains references to functions that can work on that Class. In other words, a Class is a data object that carries with it instructions on how to do operations on it, from simple things like add and subtract all the way up to complicated operations such as graphing.
				</p>

				<p>
					In the case of the tm package, the Corpus Class defines the most fundamental object that text miners care about, a corpus containing a collection of documents. Once we have our texts stored in a Corpus, the many functions that the tm package provides to us are available. The last three commands in the group above show the use of the tm_map() function, which is one of the powerful capabilities provided by tm. In each case where we call the tm_map() function, we are providing tweetCorpus as the input data, and then we are providing a command that undertakes a transformation on the corpus. We have done three transformations here, first making all of the letters lowercase, then removing the punctuation, and finally taking out the so-called "stop" words.
				</p>

				<p>
					The stop words deserve a little explanation. Researchers who developed the early search engines for electronic databases found that certain words interfered with how well their search algorithms worked. Words such as "the," "a," and "at" appeared so commonly in so many different parts of the text that they were useless for differentiating between documents. The unique and unusual nouns, verbs, and adjectives that appeared in a document did a much better job of setting a document apart from other documents in a corpus, such that researchers decided that they should filter out all of the short, commonly used words. The term "stop words" seems to have originated in the 1960s to signify words that a computer processing system would throw out or "stop using" because they had little meaning in a data processing task. To simplify the removal of stop words, the tm package contains lists of such words for different languages. In the last command on the previous page we requested the removal of all of the common stop words.
				</p>
</section>

  <section xml:id="ch13-from-corpus-to-cloud">
    <title>From Corpus to Cloud</title>
				<p>
					At this point we have processed our corpus into a nice uniform "bag of words" that contains no capital letters, punctuation, or stop words. We are now ready to conduct a kind of statistical analysis of the corpus by creating what is known as a "term-document matrix." The following command from the tm package creates the matrix:
				</p>

				<p>
					&gt; tweetTDM&lt;-TermDocumentMatrix(tweetCorpus)
				</p>

				<p>
					&gt; tweetTDM
				</p>

				<p>
					A term-document matrix (375 terms, 100 documents)
				</p>

				<p>
					Non-/sparse entries: 610/36890
				</p>

				<p>
					Sparsity : 98%
				</p>

				<p>
					Maximal term length: 21
				</p>

				<p>
					Weighting : term frequency (tf)
				</p>

				<p>
					A term-document matrix, also sometimes called a <idx>document-term matrix</idx><term>document-term matrix</term>, is a rectangular data structure with terms as the rows and documents as the columns (in other uses you may also make the terms as columns and documents as rows). A term may be a single word, for example, "biology," or it could also be a compound word, such as "data analysis." The process of determining whether words go together in a compound word can be accomplished statistically by seeing which words commonly go together, or it can be done with a dictionary. The tm package supports the dictionary approach, but we have not used a dictionary in this example. So if a term like "data" appears once in the first document, twice in the second document, and not at all in the third document, then the column for the term data will contain 1, 2, 0.
				</p>

				<p>
					The statistics reported when we ask for tweetTDM on the command line give us an overview of the results. The TermDocumentMatrix() function extracted 375 different terms from the 100 tweets. The resulting matrix mainly consists of zeros: Out of 37,500 cells in the matrix, only 610 contain non-zero entries, while 36,890 contain zeros. A zero in a cell means that that particular term did not appear in that particular document. The maximal term length was 21 words, which an inspection of the input tweets indicates is also the maximum word length of the input tweets. Finally, the last line, starting with "Weighting" indicates what kind of statistic was stored in the term-document matrix. In this case we used the default, and simplest, option which simply records the count of the number of times a term appears across all of the documents in the corpus. You can peek at what the term-document matrix contains by using the inspect function:
				</p>

				<p>
					inspect(tweetTDM)
				</p>

				<p>
					Be prepared for a large amount of output. Remember the term "sparse" in the summary of the matrix? Sparse refers to the overwhelming number of cells that contain zero indicating that the particular term does not appear in a given document. Most term document matrices are quite sparse. This one is 98% sparse because 36890/37500 = 0.98. In most cases we will need to cull or filter the term-document matrix for purposes of presenting or visualizing it. The tm package provides several methods for filtering out sparsely used terms, but in this example we are going to leave the heavy lifting to the word cloud package.
				</p>

				<p>
					As a first step we need to install and library() the "wordcloud" package. As with other packages, either use the package interface in R-Studio or the EnsurePackage() function that we wrote a few chapters ago. The wordcloud package was written by freelance statistician Ian Fellows, who also developed the "<idx>deducer</idx><term>Deducer</term>" user interface for R. Deducer provides a graphical interface that allows users who are more familiar with SPSS or SAS menu systems to be able to use R without resorting to the command line.
				</p>

				<p>
					Once the wordcloud package is loaded, we need to do a little preparation to get our data ready to submit to the word cloud generator function. That function expects two vectors as input arguments, the first a list of the terms, and the second a list of the frequencies of occurrence of the terms. The list of terms and frequencies must be sorted with the most frequent terms appearing first. To accomplish this we first have to coerce our tweet data back into a plain data matrix so that we can sort it by frequency. The first command below accomplishes this:
				</p>

				<p>
					&gt; tdMatrix &lt;- as.matrix(tweetTDM)
				</p>

				<p>
					&gt; sortedMatrix &lt;- sort(rowSums(tdMatrix),+
				</p>

				<p>
					decreasing=TRUE)
				</p>

				<p>
					&gt; cloudFrame&lt;-data.frame( +
				</p>

				<p>
					word=names(sortedMatrix),freq=sortedMatrix)
				</p>

				<p>
					&gt; wordcloud(cloudFrame$word,cloudFrame$freq)
				</p>

				<p>
					In the next command above, we are accomplishing two things in one command: We are calculating the sums across each row, which gives us the total frequency of a term across all of the different tweets/documents. We are also sorting the resulting values with the highest frequencies first. The result is a named list: Each item of the list has a frequency and the name of each item is the term to which that frequency applies.
				</p>

				<p>
					In the second to last command above, we are extracting the names from the named list in the previous command and binding them together into a dataframe with the frequencies. This dataframe, "cloudFrame", contains exactly the same information as the named list. "sortedMatrix," but cloudFrame has the names in a separate column of data. This makes it easier to do the final command above, which is the call to the wordcloud() function. The wordcloud() function has lots of optional parameters for making the word cloud more colorful, controlling its shape, and controlling how frequent an item must be before it appears in the cloud, but we have used the default settings for all of these parameters for the sake of simplicity. We pass to the wordcloud() function the term list and frequency list that we bound into the dataframe and wordcloud() produces the nice graphic that you see below.
				</p>

				<image source="solar-energy-cloud.png"/>

				<p>
					If you recall the Twitter search that we used to retrieve those tweets (#solar) it makes perfect sense that "solar" is the most frequent term (even though we filtered out all of the hashtags. The next most popular term is "energy" and after that there are a variety of related words such as "independence," "green," "wind," and "metering."
				</p>
</section>

  <section xml:id="ch13-chapter-challenge">
    <title>Chapter Challenge</title>

				<p>
					Develop a function that builds upon previous functions we have developed, such as TweetFrame() and CleanTweets(), to take a search term, conduct a Twitter search, clean up the resulting texts, formulate a term-document matrix, and submit resulting term frequencies to the wordcloud() function. Basically this would be a "turnkey" package that would take a Twitter search term and produce a word cloud from it, much like the Jason Davies site described at the beginning of this chapter.
				</p>

			</section>

			<section xml:id="sources-used-in-this-chapter">
				<title>Sources Used in This Chapter </title>

				<p><ul>
					<li>
									
														<p>
										<url href="http://cran.r-project.org/web/packages/wordcloud/wordcloud.pdf">http://cran.r-project.org/web/packages/wordcloud/wordcloud.pdf</url>
									</p>
									
					</li>

					<li>
									
														<p>
										<url href="http://en.wikipedia.org/wiki/Document-term_matrix">http://en.wikipedia.org/wiki/Document-term_matrix</url>
									</p>
									
					</li>

					<li>
									
														<p>
										<url href="http://en.wikipedia.org/wiki/Stop_words">http://en.wikipedia.org/wiki/Stop_words</url>
									</p>
									
					</li>

					<li>
									
														<p>
										<url href="http://en.wikipedia.org/wiki/Text_mining">http://en.wikipedia.org/wiki/Text_mining</url>
									</p>
									
					</li>
					<li>
									
														<p>
										<url href="http://www.jasondavies.com/wordcloud/">http://www.jasondavies.com/wordcloud/</url>
									</p>
									
					</li>

				</ul></p>

			</section>

			<section xml:id="r-code-for-cleantweets-function">
				<title>R Code for CleanTweets() Function </title>

				<p>
					# CleanTweets() Takes the junk out of a vector of
				</p>

				<p>
					# tweet texts
				</p>

				<p>
					CleanTweets&lt;-function(tweets)
				</p>

				<p>
					{
				</p>

				<p>
					# Remove redundant spaces
				</p>

				<p>
					tweets &lt;str_replace_all(tweets," "," ")
				</p>

				<p>
					# Get rid of URLs
				</p>

				<p>
					tweets &lt;- str_replace_all(tweets, +
				</p>

				<p>
					"http://t.co/[a-z,A-Z,0-9]{8}","")
				</p>

				<p>
					# Take out retweet header, there is only one
				</p>

				<p>
					tweets &lt;- str_replace(tweets,"RT @[a-z,A-Z]*: ","")
				</p>

				<p>
					# Get rid of hashtags
				</p>

				<p>
					tweets &lt;- str_replace_all(tweets,"#[a-z,A-Z]*","")
				</p>

				<p>
					# Get rid of references to other screennames
				</p>

				<p>
					return(tweets)
				</p>

				<p>
					}
				</p>

				<p>
					tweets &lt;- str_replace_all(tweets,"@[a-z,A-Z]*","")
				</p>
  </section>
</chapter>