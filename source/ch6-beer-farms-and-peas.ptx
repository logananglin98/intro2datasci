<?xml version="1.0" encoding="UTF-8"?>

<chapter xml:id="ch6-beer-farms-and-peas" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Beer, Farms, and Peas</title>



  <subsection xml:id="beer-farms-and-peas">
    <title>Beer, Farms, and Peas</title>
  <introduction>
  <figure xml:id="peas-on-x-y-axes">
          <image source="peas-graph.png"/>
          <caption>Peas on x-y axes</caption>
  </figure>



				<p>
					<em>Many of the simplest and most practical methods for summarizing collections of numbers come to us from four guys who were born in the 1800s at the start of the industrial revolution. A considerable amount of the work they did was focused on solving real world problems in manufacturing and agriculture by using data to describe and draw inferences from what they observed.</em>
				</p>
  </introduction>
				<p>
					The end of the 1800s and the early 1900s were a time of astonishing progress in mathematics and science. Given enough time, paper, and pencils, scientists and mathematicians of that age imagined that just about any problem facing humankind including the limitations of people themselves could be measured, broken down, analyzed, and rebuilt to become more efficient. Four Englishmen who epitomized both this scientific progress and these idealistic beliefs were Francis Galton, Karl Pearson, William Sealy Gosset, and Ronald Fisher.
				</p>

				<p>
					First on the scene was Francis Galton, a half-cousin to the more widely known Charles Darwin, but quite the intellectual force himself. Galton was an English gentleman of independent means who studied Latin, Greek, medicine, and mathematics, and who made a name for himself as an African explorer. He is most widely known as a proponent of "eugenics," and is credited with coining the term. Eugenics is the idea that the human race could be improved through selective breeding. Galton studied heredity in peas, rabbits, and people and concluded that certain people should be paid to get married and have children because their offspring would improve the human race. These ideas were later horribly misused in the 20th century, most notably by the Nazis as a justification for killing people because they belonged to supposedly inferior races. Setting eugenics aside, however, Galton made several notable and valuable contributions to mathematics and statistics, in particular illuminating two basic techniques that are widely used today: correlation and regression.
				</p>

				<p>
					For all his studying and theorizing, Galton was not an outstanding mathematician, but he had a junior partner, Karl Pearson, who is often credited with founding the field of mathematical statistics.
				</p>

				<p>
					Pearson refined the math behind correlation and regression and did a lot else besides to contribute to our modern abilities to manage numbers. Like Galton, Pearson was a proponent of eugenics, but he also is credited with inspiring some of Einstein’s thoughts about relativity and was an early advocate of women’s rights.
				</p>

				<p>
					Next to the statistical party was William Sealy Gosset, a wizard at both math and chemistry. It was probably the latter expertise that led the Guinness Brewery in Dublin Ireland to hire Gosset after college. As a forward looking business, the Guinness brewery was on the lookout for ways of making batches of beer more consistent in quality. Gosset stepped in and developed what we now refer to as small sample statistical techniques, a way of generalizing from the results of a relatively few observations. Of course, brewing a batch of beer is a time consuming and expensive process, so in order to draw conclusions from experimental methods applied to just a few batches, Gosset had to figure out the role of chance in determining how a batch of beer had turned out. Guinness frowned upon academic publications, so Gosset had to publish his results under the modest pseudonym, "Student." If you ever hear someone discussing the "Student’s t-Test," that is where the name came from.
				</p>

				<p>
					Last but not least among the born-in-the-1800s bunch was Ronald Fisher, another mathematician who also studied the natural sciences, in his case biology and genetics. Unlike Galton, Fisher was not a gentleman of independent means, in fact, during his early married life he and his wife struggled as subsistence farmers. One of Fisher’s professional postings was to an agricultural research farm called Rothhamsted Experimental Station. Here, he had access to data about variations in crop yield that led to his development of an essential statistical technique known as the analysis of variance. Fisher also pioneered the area of experimental design, which includes matters of factors, levels, experimental groups, and control groups that we noted in the previous chapter.
				</p>

				<p>
					Of course, these four are certainly not the only 19th and 20th century mathematicians to have made substantial contributions to practical statistics, but they are notable with respect to the applications of mathematics and statistics to the other sciences (and "Beer, Farms, and Peas" makes a good chapter title as well).
				</p>

				<p>
					One of the critical distinctions woven throughout the work of these four is between the "sample" of data that you have available to analyze and the larger "population" of possible cases that may or do exist. When Gosset ran batches of beer at the brewery, he knew that it was impractical to run every possible batch of beer with every possible variation in recipe and preparation. Gosset knew that he had to run a few batches, describe what he had found and then generalize or infer what might happen in future batches. This is a fundamental aspect of working with all types and amounts of data: Whatever data you have, there’s always more out there. There’s data that you might have collected by changing the way things are done or the way things are measured. There’s future data that hasn’t been collected yet and might never be collected. There’s even data that we might have gotten using the exact same strategies we did use, but that would have come out subtly different just due to randomness. Whatever data you have, it is just a snapshot or "sample" of what might be out there. This leads us to the conclusion that we can never, ever 100% trust the data we have. We must always hold back and keep in mind that there is always uncertainty in data. A lot of the power and goodness in statistics comes from the capabilities that people like Fisher developed to help us characterize and quantify that uncertainty and for us to know when to guard against putting too much stock in what a sample of data have to say. So remember that while we can always <term>describe</term> the sample of data we have, the real trick is to <term>infer</term> what the data may mean when generalized to the larger population of data that we don’t have. This is the key distinction between descriptive and inferential statistics.
				</p>

				<p>
					We have already encountered several descriptive statistics in previous chapters, but for the sake of practice here they are again, this time with the more detailed definitions:
				</p>

				<p><ul>
					<li>
									<blockquote>
														<p>
										The <term>mean</term> (technically the arithmetic mean), a measure of central tendency that is calculated by adding together all of the observations and dividing by the number of observations.
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										The <term>median</term>, another measure of central tendency, but one that cannot be directly calculated. Instead, you make a sorted list of all of the observations in the sample, then go halfway up that list. Whatever the value of the observation is at the halfway point, that is the median.
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										The <term>range</term>, which is a measure of "dispersion" how spread out a bunch of numbers in a sample are calculated by subtracting the lowest value from the highest value.
									</p>
									</blockquote>
					</li>

				</ul></p>

				<p>
					To this list we should add three more that you will run into in a variety of situations:
				</p>

				<p><ul>
					<li>
									<blockquote>
														<p>
										The <term>mode</term>, another measure of central tendency. The mode is the value that occurs most often in a sample of data. Like the median, the mode cannot be directly calculated. You just have to count up how many of each number there are and then pick the category that has the most.
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										The <term>variance</term>, a measure of dispersion. Like the range, the variance describes how spread out a sample of numbers is. Unlike the range, though, which just uses two numbers to calculate dispersion, the variance is obtained from all of the numbers through a simple calculation that compares each number to the mean. If you remember the ages of the family members from the previous chapter and the mean age of 22, you will be able to make sense out of the following table:
									</p>
									</blockquote>
					</li>

				</ul></p>

				<table>
					<title></title>
					<tabular>
					<row header="yes">
						<cell halign="left"><term>WHO</term></cell>
						<cell halign="left"><term>AGE</term></cell>
						<cell halign="left"><term>AGE-MEAN</term></cell>
						<cell halign="left"><term>(AGEMEAN)</term></cell>
					</row>
					<row class="odd">
						<cell halign="left">Dad</cell>
						<cell halign="left">43</cell>
						<cell halign="left">43-22=21</cell>
						<cell halign="left">21*21=441</cell>
					</row>
					<row class="even">
						<cell halign="left">Mom</cell>
						<cell halign="left">42</cell>
						<cell halign="left">42-22=20</cell>
						<cell halign="left">20*20=400</cell>
					</row>
					<row class="odd">
						<cell halign="left">Sis</cell>
						<cell halign="left">12</cell>
						<cell halign="left">12-22=-10</cell>
						<cell halign="left">-10*-10=100</cell>
					</row>
					<row class="even">
						<cell halign="left">Bro</cell>
						<cell halign="left">8</cell>
						<cell halign="left">8-22=-14</cell>
						<cell halign="left">-14*-14=196</cell>
					</row>
					<row class="odd">
						<cell halign="left">Dog</cell>
						<cell halign="left">5</cell>
						<cell halign="left">5-22=-17</cell>
						<cell halign="left">-17*-17=289</cell>
					</row>
					<row class="even">
						<cell halign="left"></cell>
						<cell halign="left"></cell>
						<cell halign="left"><term>Total:</term></cell>
						<cell halign="left"><term>1426</term></cell>
					</row>
					<row class="odd">
						<cell halign="left"></cell>
						<cell halign="left"></cell>
						<cell halign="left"><term>Total/4:</term></cell>
						<cell halign="left"><term>356.5</term></cell>
					</row>
					</tabular>
				</table>

				<p>
					This table shows the calculation of the variance, which begins by obtaining the "deviations" from the mean and then "squares" them (multiply each times itself) to take care of the negative deviations (for example, -14 from the mean for Bro). We add up all of the squared deviations and then divide by the number of observations to get a kind of "average squared deviation." Note that it was not a mistake to divide by 4 instead of 5 the reasons for this will become clear later in the book when we examine the concept of degrees of freedom. This result is the <term>variance</term>, a very useful mathematical concept that appears all over the place in statistics. While it is mathematically useful, it is not too nice to look at. For instance, in this example we are looking at the 356.5 squared-years of deviation from the mean. Who measures anything in squared years? Squared feet maybe, but that’s a different discussion. So, to address this weirdness, statisticians have also provided us with:
				</p>

				<p><ul>
					<li>
									<blockquote>
														<p>
										The <term>standard deviation</term>, another measure of dispersion, and a cousin to the variance. The standard deviation is simply the square root of the variance, which puts us back in regular units like "years." In the example above, the standard deviation would be about 18.88 years (rounding to two decimal places, which is plenty in this case).
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										Standard deviation has a complicated formula, but you can think of it as “the average distance from the mean.” In the example above, the average distance to the mean is about 18.88 years.
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										Intuitively, you can think about both the standard deviation and the variance as measuring how spread out the data is. When these numbers are 0 or very small compared to the mean, the data is not very spread out.
									</p>
									</blockquote>
					</li>

				</ul></p>

				<p>
					Now let’s have R calculate some statistics for us:
				</p>

				<p>
					&gt; var(myFam$myFamAge)
				</p>

				<p>
					[1] 356.5
				</p>

				<p>
					&gt; sd(myFam$myFamAge)
				</p>

				<p>
					[1] 18.88121
				</p>

				<p>
					Note that these commands carry on using the data used in the previous chapter, including the use of the $ to address variables within a dataframe. If you do not have the data from the previous chapter you can also do this:
				</p>

				<p>
					&gt; var(c(43,42,12,8,5))
				</p>

				<p>
					[1] 356.5
				</p>

				<p>
					&gt; sd(c(43,42,12,8,5))
				</p>

				<p>
					[1] 18.88121
				</p>

				<p>
					This is a pretty boring example, though, and not very useful for the rest of the chapter, so here’s the next step up in looking at data. We will use the Windows or Mac clipboard to cut and paste a larger data set into R. Go to the U.S. Census website where they have stored population data: <url href="https://www.census.gov/data/tables/time-series/demo/popest/2010s-state-total.html">US Census Bureau: State Population Totals: 2010-2019</url>
				</p>

				<p>
					Assuming you have a spreadsheet program available, click on the XLS link for "<url href="https://www2.census.gov/programs-surveys/popest/tables/2010-2019/state/totals/nst-est2019-01.xlsx">Annual Estimates of the Resident Population for the United States, Regions, States, and Puerto Rico: April 1, 2010 to July 1, 2019 (NST-EST2019-01)</url> " When the spreadsheet is open, select the population estimates for the fifty states. The first few looked like this in the 2011 data:
				</p>

				<p>
					.Alabama 4,799,069
				</p>

				<p>
					.Alaska 722,128
				</p>

				<p>
					.Arizona 6,472,643
				</p>

				<p>
					.Arkansas 2,940,667
				</p>

				<p>
					Before you copy the numbers, take out the commas by switching the cell type to "General." This can usually be accomplished under the Format menu, but you might also have a toolbar button to do the job.
				</p>

				<figure>
	<image source="dropdown-general.png" width="60%" height="60%"/>
					<caption>Excel interface showing how to change cell formatting to 'General' to remove commas from population data.</caption>
</figure>

				<p>
					Generally, you want to keep the names of columns for the data, however, for the purposes of learning just delete everything and only keep population data for the 50 states (and Washington DC). You should only have the numeric data, occupying a total of 51 lines:
				</p>

				<figure>
	<image source="numbers.png"/>
					<caption>Spreadsheet displaying U.S. state population data in numeric format without labels or commas, ready for import into R.</caption>
</figure>

				<p>
					Then open R Studio Cloud and Exploring Populations Homework. In the bottom right window click on the Upload button:
				</p>

				<figure>
	<image source="R-window.png"/>
					<caption>R Studio interface highlighting the Upload button in the Files pane for uploading datasets.</caption>
</figure>

				<p>
					Select the XSL file that you downloaded and edited and upload it. It should now appear in the Files window. Next, click on the file and select Import Dataset
				</p>

				<p>
					You might be prompted to a page that requires installation of extensions to your Cloud environment. Click yes.
				</p>

				<p>
					After installation of readxl and Rcpp packages for smooth integration of XSL files in our environment, you will be prompted to the Import Excel Data window. There are three changes that you will need to make 1. Change the name of the variable to which you are assigning the data from this excel sheet from its original name to USstatePops. 2. Uncheck “First Row as Names” box as we do not want the population of Alabama over different years to become the name for our columns 3. (optional) Uncheck Open Data Viewer box unless of course you are curious to see the Data Viewer window.
				</p>

				<figure>
	<image source="import-excel.png"/>
					<caption>Import Excel Data window in R Studio, showing settings for importing state population data as a dataframe.</caption>
</figure>

				<p>
					This is what the Import Excel Data window should look like after:
				</p>

				<figure>
	<image source="readxl.png"/>
					<caption>R Studio import screen showing the final setup before importing the USstatePops dataset using the readxl package.</caption>
</figure>

				<p>
					Nice work, you have successfully uploaded your first data set into R Studio Cloud.
				</p>

				<p>
					Only the first three observations are shown in order to save space on this page. Your output R should show the whole list.
				</p>

				<p>
					This would be a great moment to practice your skills from the previous chapter by using the str() and summary() functions on our new data object called USstatePops. Did you notice anything interesting from the results of these functions? One thing you might have noticed is that there are 51 observations instead of 50. Can you guess why? If not, go back and look at your original data from the spreadsheet or the U.S. Census site. The other thing you may have noticed is that USstatePops is a dataframe, and not a plain vector of numbers. You can actually see this in the output above: In the second command line where we request that R reveal what is stored in USstatePops, it responds with a column topped by the designation "V1" or “...1”. Because we did not give R any information about the numbers it read in from the clipboard, it called them "V1" or “...1”, short for Variable One, by default. So anytime we want to refer to our list of population numbers we actually have to use the name USstatePops$V1. If this sounds unfamiliar, take another look at the previous "Rows and Columns" chapter for more information on addressing the columns in a dataframe.
				</p>

				<p>
					Now we’re ready to have some fun with a good sized list of numbers. Here are the basic descriptive statistics on the population of the states:
				</p>

				<p>
					&gt; mean(USstatePops$...1)
				</p>

				<p>
					[1] 6053834
				</p>

				<p>
					&gt; median(USstatePops$...1)
				</p>

				<p>
					[1] 4339367
				</p>

				<p>
					&gt; mode(USstatePops$...1)
				</p>

				<p>
					[1] "numeric"
				</p>

				<p>
					&gt; var(USstatePops$...1)
				</p>

				<p>
					[1] 4.656676e+13
				</p>

				<p>
					&gt; sd(USstatePops$...1)
				</p>

				<p>
					[1] 6823984
				</p>

				<p>
					Some great summary information there, but wait, a couple things have gone awry:
				</p>

				<p><ul>
					<li>
									<blockquote>
														<p>
										The mode() function has returned the data type of our vector of numbers instead of the statistical mode. This is weird but true: the basic R package does not have a statistical mode function! This is partly due to the fact that the mode is only useful in a very limited set of situations, but we will find out in later chapters how add-on packages can be used to get new functions in R including one that calculates the statistical mode.
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										The variance is reported as 4.656676e+13. This is the first time that we have seen the use of scientific notation in R. If you haven’t seen this notation before, the way you interpret it is to imagine 4.656676 multiplied by 10,000,000,000,000 (also known as 10 raised to the 13<sup>th</sup> power). You can see that this is ten trillion, a huge and unwieldy number, and that is why scientific notation is used. If you would prefer not to type all of that into a calculator, another trick to see what number you are dealing with is just to move the decimal point 13 digits to the right.
									</p>
									</blockquote>
					</li>

				</ul></p>

				<p>
					Other than these two issues, we now know that the average population of a U.S. state is 6,053,834 with a standard deviation of 6,823,984. You may be wondering, though, what does it mean to have a standard deviation of almost seven million? The mean and standard deviation are OK, and they certainly are mighty precise, but for most of us, it would make much more sense to have a <em>picture</em> that shows the central tendency and the dispersion of a large set of numbers. So here we go. Run this command:
				</p>

				<p>
					hist(USstatePops$...1)
				</p>

				<p>
					Here’s the output you should get:
				</p>

				<figure>
	<image source="histogram-US-pop.png"/>
					<caption>Histogram of USstatePops$V1</caption>
</figure>

				<p>
					<term>Histogram of USstatePops$...1</term>
				</p>

				<p>
					A histogram is a specialized type of bar graph designed to show "frequencies." Frequencies means how often a particular value or range of values occurs in a dataset. This histogram shows a very interesting picture. There are nearly 30 states with populations under five million, another 10 states with populations under 10 million, and then a very small number of states with populations greater than 10 million. Having said all that, how do we glean this kind of information from the graph? First, look along the Y-axis (the vertical axis on the left) for an indication of how often the data occur. The tallest bar is just to the right of this and it is nearly up to the 30 mark. To know what this tall bar represents, look along the X-axis (the horizontal axis at the bottom) and see that there is a tick mark for every two bars. We see scientific notation under each tick mark. The first tick mark is 1e+07, which translates to 10,000,000. So each new bar (or an empty space where a bar would go) goes up by five million in population. With these points in mind it should now be easy to see that there are nearly 30 states with populations under five million.
				</p>

				<p>
					If you think about presidential elections, or the locations of schools and businesses, or how a single U.S. state might compare with other countries in the world, it is interesting to know that there are two really giant states and then lots of much smaller states. Once you have some practice reading histograms, all of the knowledge is available at a glance.
				</p>

				<p>
					On the other hand there is something unsatisfying about this diagram. With over forty of the states clustered into the first couple of bars, there might be some more details hiding in there that we would like to know about. This concern translates into the number of bars shown in the histogram. There are eight shown here, so why did R pick eight?
				</p>

				<p>
					The answer is that the hist() function has an algorithm or recipe for deciding on the number of categories/bars to use by default. The number of observations and the spread of the data and the amount of empty space there would be are all taken into account. Fortunately it is possible and easy to ask R to use more or fewer categories/bars with the "breaks" parameter, like this:
				</p>

				<p>
					hist(USstatePops$...1, breaks=20)
				</p>

				<figure>
	<image source="histogram-US-pops.png"/>
					<caption>Histogram of USstatePops$V1 </caption>
</figure>

				<p>
					<term>Histogram of USstatePops$...1</term>
				</p>

				<p>
					This gives us five bars per tick mark or about two million for each bar. So the new histogram above shows very much the same pattern as before: 15 states with populations under two million. The pattern that you see here is referred to as a distribution. This is a distribution that starts off tall on the left and swoops downward quickly as it moves to the right. You might call this a "reverse-J" distribution because it looks a little like the shape a J makes, although flipped around vertically. More technically this could be referred to as a Pareto distribution (named after the economist Vilfredo Pareto). We don’t have to worry about why it may be a Pareto distribution at this stage, but we can speculate on why the distribution looks the way it does. First, you can’t have a state with no people in it, or worse yet negative population. It just doesn’t make any sense. So a state has to have at least a few people in it, and if you look through U.S. history every state began as a colony or a territory that had at least a few people in it. On the other hand, what does it take to grow really large in population? You need a lot of land, first of all, and then a good reason for lots of people to move there or lots of people to be born there. So there are lots of limits to growth: Rhode Island is too small to have a bazillion people in it and Alaska, although it has tons of land, is too cold for lots of people to want to move there. So all states probably started small and grew, but it is really difficult to grow really huge. As a result we have a distribution where most of the cases are clustered near the bottom of the scale and just a few push up higher and higher. But as you go higher, there are fewer and fewer states that can get that big, and by the time you are out at the end, just shy of 40 million people, there’s only one state that has managed to get that big. By the way, do you know or can you guess what that humongous state is?
				</p>

				<p>
					There are lots of other distribution shapes. The most common one that almost everyone has heard of is sometimes called the "bell" curve because it is shaped like a bell. The technical name for this is the normal distribution. The term "normal" was first introduced by Carl Friedrich Gauss (1777-1855), who supposedly called it that in a belief that it was the most typical distribution of data that one might find in natural phenomena. The following histogram depicts the typical bell shape of the normal distribution.
				</p>

				<figure>
	<image source="histogram-rnorm.png"/>
					<caption>Histogram of rnorm(51, 6053834, 6823984) </caption>
</figure>

				<p>
					<term>Histogram of rnorm(51, 6053834, 6823984)</term>
				</p>

				<p>
					If you are curious, you might be wondering how R generated the histogram above, and, if you are alert, you might notice that the histogram that appears above has the word "rnorm" in a couple of places. Here’s another of the cool features in R: it is incredibly easy to generate "fake" data to work with when solving problems or giving demonstrations. The data in this histogram were generated by R’s rnorm() function, which generates a random data set that fits the normal distribution (more closely if you generate a lot of data, less closely if you only have a little). Some further explanation of the rnorm() command will make sense if you remember that the state population data we were using had a mean of 6,053,834 and a standard deviation of 6,823,984. The command used to generate this histogram was:
				</p>

				<p>
					hist(rnorm(51, 6043834, 6823984))
				</p>

				<p>
					There are two very important new concepts introduced here. The first is a nested function call: The hist() function that generates the graph "surrounds" the rnorm() function that generates the new fake data. (Pay close attention to the parentheses!) The inside function, rnorm(), is run by R first, with the results of that sent directly and immediately into the hist() function.
				</p>

				<p>
					The other important thing is the "arguments that" were "passed" to the rnorm() function. "Argument" is a term used by computer scientists to refer to some extra information that is sent to a function to help it know how to do its job. In this case we passed three arguments to rnorm() that it was expecting in this order: the number of observations to generate in the fake dataset, the mean of the distribution, and the standard deviation of the distribution. The rnorm() function used these three numbers to generate 51 random data points that, roughly speaking, fit the normal distribution. So the data shown in the histogram above are an approximation of what the distribution of state populations might look like if,
				</p>

				<p>
					instead of being reverse-J-shaped (Pareto distribution), they were normally distributed.
				</p>

				<p>
					The normal distribution is used extensively through applied statistics as a tool for making comparisons. For example, look at the rightmost bar in the previous histogram. The label just to the right of that bar is 3e+07, or 30,000,000. We already know from our real state population data that there is only one actual state with a population in excess of 30 million (if you didn’t look it up, it is California). So if all of a sudden, someone mentioned to you that he or she lived in a state, <em>other than</em> California, that had 30 million people, you would automatically think to yourself, "Wow, that’s unusual and I’m not sure I believe it." And the reason that you found it hard to believe was that you had a distribution to compare it to. Not only did that distribution have a characteristic shape (for example, J-shaped, or bell shaped, or some other shape), it also had a center point, which was the mean, and a "spread," which in this case was the standard deviation. Armed with those three pieces of information, the type/shape of distribution, an anchoring point, and a spread (also known as the amount of variability), you have a powerful tool for making comparisons.
				</p>

				<p>
					In the next chapter we will conduct some of these comparisons to see what we can infer about the ways things are in general, based on just a subset of available data, or what statisticians call a sample.
				</p>

				<p>
					<term>Chapter Challenge</term>
				</p>

				<p>
					In this chapter, we used rnorm() to generate random numbers that closely fit a normal distribution. We also learned that the state population data was a "Pareto" distribution. Do some research to find out what R function generates random numbers using the Pareto distribution. Then run that function with the correct parameters to generate 51 random numbers (hint: experiment with different probability values). Create a histogram of these random numbers and describe the shape of the distribution.
				</p>

				<p>
					<term>Sources</term>
				</p>

				<p><ul>
					<li>
									<blockquote>
														<p>
										<url href="http://en.wikipedia.org/wiki/Carl_Friedrich_Gauss">Carl Friedrich Gauss</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="http://en.wikipedia.org/wiki/Francis_Galton">Francis Galton</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="http://en.wikipedia.org/wiki/Pareto_distribution">Pareto distribution</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="http://en.wikipedia.org/wiki/Karl_Pearson">Karl Pearson</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="http://en.wikipedia.org/wiki/Ronald_Fisher">Ronald Fisher</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="http://en.wikipedia.org/wiki/William_Sealy_Gosset">William Sealy Gosset</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="http://en.wikipedia.org/wiki/Normal_distribution">Normal Distribution</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="https://www.census.gov/data/tables/time-series/demo/popest/2010s-state-total.html">US Census: State Population Totals and Components of Change: 2010-2019</url>
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										<url href="http://www.r-tutor.com/sitemap">R Tutorial: Site Map</url>
									</p>
									</blockquote>
					</li>

				</ul></p>

			</subsection>

			<subsection xml:id="r-functions-used-in-this-chapter-1">
				<title>R Functions Used in This Chapter </title>

				<p><ul>
					<li>
									<blockquote>
														<p>
										mean() Calculate arithmetic mean
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										median() Locate the median
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										mode() Tells the data type/mode of a data object.<term> Important note</term>: Surprisingly, this mode() is NOT a statistical mode. In fact, R does not have a standard in-built function to calculate the statistical mode.
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										var() Calculate the sample variance
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										sd() Calculate the sample standard deviation
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										hist() Produces a histogram graphic
									</p>
									</blockquote>
					</li>

				</ul></p>

			</subsection>

			<subsection xml:id="if-all-else-fails">
				<title>If All Else Fails </title>

				<p>
					In case you have difficulty with the read.DIF() or read.table() functions, the code shown below can be copied and pasted (or, in the worst case scenario, typed) into the R console to create the data set used in this chapter.
				</p>

				<p>
					V1 &lt;- c(4779736,710231,6392017,2915918,37253956, 5029196,3574097,897934,601723,18801310,9687653, 1360301,1567582,12830632,6483802,3046355,2853118, 4339367,4533372,1328361,5773552,6547629,9883640, 5303925,2967297,5988927,989415,1826341,2700551, 1316470,8791894,2059179,19378102,9535483,672591, 11536504,3751351,3831074,12702379,1052567, 4625364,814180,6346105,25145561,2763885,625741, 8001024,6724540,1852994,5686986,563626)
				</p>

				<p>
					USstatePops &lt;- data.frame(V1)
				</p>

				<p>
					<term>Question:</term> A bar graph that displays the frequencies of occurrence for a numeric variable is called a
				</p>

				<p><ol>
					<li>
									<blockquote>
														<p>
										Histogram
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										Pictogram
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										Bar Graph
									</p>
									</blockquote>
					</li>

					<li>
									<blockquote>
														<p>
										Bar Chart
									</p>
									</blockquote>
					</li>

				</ol></p>
      </subsection>

</chapter>